1. without Deep Supervision
2. dice_coef_loss as training loss


def Unet_scheduler(epoch, lr):
    if epoch < 3:
        return lr
    elif epoch < 10:
        return 5e-4
    elif epoch < 16:
        return 2e-4
    else:
        return lr * tf.math.exp(-0.05)

EPOCHES = 50
    LR = 8e-4
    STEPS = 300
    BATCH_SIZE = 1
    TARGET_SIZE = (512, 512)

data_gen_args_3 = dict(  # rescale=1. / 255,
        rotation_range=2,
        width_shift_range=0.05,
        height_shift_range=0.05,
        shear_range=0.05,
        zoom_range=0.05,
        horizontal_flip=True,
        vertical_flip=True,
        fill_mode='constant')

这个setting预期为最好setting，相比setting2优势为  TARGET_SIZE = (512, 512)， 多了vertical_flip=True, rotation_range=2,图片多样性更好，实际效果却不理想.
为什么不使用(1-MeanIOU)作为training loss:
Intersection over Union (IoU) is the most popular evaluation
metric used in the object detection benchmarks. However,
there is a gap between optimizing the commonly used
distance losses for regressing the parameters of a bounding
box and maximizing this metric value. The optimal objective
for a metric is the metric itself. In the case of axisaligned
2D bounding boxes, it can be shown that IoU can
be directly used as a regression loss. However, IoU has a
plateau making it infeasible to optimize in the case of nonoverlapping
bounding boxes.